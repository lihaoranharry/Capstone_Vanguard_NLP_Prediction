{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import requests\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import nltk\n",
    "from nltk import word_tokenize, everygrams\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def single_read_in_text(file_path,file_name):\n",
    "    '''\n",
    "    input:\n",
    "    file_path: path of a individule csv file e.g. location of 21centfoxinc_sec_files.csv\n",
    "    file_name: name of the file e.g 21centfoxinc_sec_files.csv\n",
    "    \n",
    "    output:\n",
    "    a sigle pandas dataframe with all the original columns from the input file + a column for the 'cleaned' data\n",
    "    \n",
    "    dev:\n",
    "    1) can add more output columns for features\n",
    "    2) can further clean the data \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## read in single csv to pandas \n",
    "    individule_csv = pd.read_csv(file_path+\"/\"+file_name)\n",
    "    raw_texts = []\n",
    "    clean_texts = []\n",
    "    \n",
    "    ## create a connection with the url link and readin the raw file\n",
    "    for url in individule_csv['sec_full_path']:\n",
    "        print(url)\n",
    "        raw_texts.append([url,request.urlopen(url).read().decode('utf8')])\n",
    "    \n",
    "    ## clean the raw file by:\n",
    "    ## 1. remove html tags \n",
    "    ## 2. break the text by \"\\n\"\n",
    "    ## 3. remove the spaces in the front of and after each \"\\n\"\n",
    "    \n",
    "    for raw_file in raw_texts:\n",
    "        \n",
    "        \n",
    "        \n",
    "        clean_texts.append([raw_file[0],\n",
    "                            '%%'.join(list(filter(None,\n",
    "                                                 [re.sub('[\\t]+', ' ', i.strip()) for \n",
    "                                                  i in BeautifulSoup(raw_file[1], \"lxml\").text.split('\\n')])))])\n",
    "        ## more columns, features, data cleanings can be put here\n",
    "    \n",
    "    ## merge back to the original read in dataframe \n",
    "    clean_texts_df = pd.DataFrame(clean_texts)\n",
    "    clean_texts_df.columns = ['sec_full_path', 'text']\n",
    "    merged_df = pd.merge(left = individule_csv, right = clean_texts_df, on = 'sec_full_path')\n",
    "    \n",
    "    ## add one more column to indicate the file name \n",
    "    merged_df['file_name'] = file_name\n",
    "    return merged_df\n",
    "def folder_read_in_text(folder_path, ext = '.csv'):\n",
    "    '''\n",
    "    input: \n",
    "    folder_path: path of a individule csv file e.g. location of 21centfoxinc_sec_files.csv\n",
    "    ext: extension of the files that are interested, default to be .csv \n",
    "    \n",
    "    output:\n",
    "    a sigle pandas dataframe with all the original columns from all the input files inside the folder\n",
    "    + a column for the 'cleaned' data\n",
    "    + a column for the file name \n",
    "    \n",
    "    Utilize the single_read_in_text function \n",
    "    '''\n",
    "    file = []\n",
    "    direc = folder_path \n",
    "\n",
    "    # Select only files with the ext extension\n",
    "    txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "    temp_df = pd.DataFrame()\n",
    "    \n",
    "    ## Utilize the single_read_in_text function to process data\n",
    "\n",
    "    for i in txt_files:\n",
    "        temp_df = temp_df.append(single_read_in_text(file_path = folder_path,file_name = i), ignore_index=True)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#single_read_in_text('C:/Users/li haoran/Desktop/New folder','21centfoxinc_sec_files.csv')\n",
    "working_file = folder_read_in_text(folder_path='C:/Users/li haoran/Documents/GitHub/Capstone_Vanguard_NLP_Prediction/Inputs',\n",
    "                                   ext = '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_file.to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/li haoran/Desktop/sec files/cleaned_data.csv'\n",
    "cleaned_data = pd.read_csv(file_path, sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "cleaned_data['only_eng_words'] = cleaned_data['text'].apply(lambda x: \" \".join(w for w in nltk.wordpunct_tokenize(x) \\\n",
    "                                      if w.lower() in words))\n",
    "cleaned_data['evy_gram_1_3'] =cleaned_data['only_eng_words'].apply(lambda x: [' '.join(ng) for ng in everygrams(word_tokenize(x), 1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th colspan=\"2\" halign=\"left\">doccount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21centfoxinc_sec_files.csv</td>\n",
       "      <td>1048</td>\n",
       "      <td>3978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attinc_sec_files.csv</td>\n",
       "      <td>555</td>\n",
       "      <td>3383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbscorp_sec_files.csv</td>\n",
       "      <td>385</td>\n",
       "      <td>2578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comcastcorp_sec_files.csv</td>\n",
       "      <td>277</td>\n",
       "      <td>2735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verizoncom_sec_files.csv</td>\n",
       "      <td>525</td>\n",
       "      <td>3319.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name doccount        \n",
       "                                 count     sum\n",
       "0  21centfoxinc_sec_files.csv     1048  3978.0\n",
       "1        attinc_sec_files.csv      555  3383.0\n",
       "2       cbscorp_sec_files.csv      385  2578.0\n",
       "3   comcastcorp_sec_files.csv      277  2735.0\n",
       "4    verizoncom_sec_files.csv      525  3319.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[['file_name', 'doccount']]\\\n",
    ".groupby(['file_name'])\\\n",
    ".agg(['count','sum'])\\\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[cleaned_data['file_name']=='21centfoxinc_sec_files.csv']\\\n",
    ".to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated_21centfoxinc_sec_files.csv', sep='|', encoding='utf-8')\n",
    "\n",
    "cleaned_data[cleaned_data['file_name']=='attinc_sec_files.csv']\\\n",
    ".to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated_attinc_sec_files.csv', sep='|', encoding='utf-8')\n",
    "\n",
    "cleaned_data[cleaned_data['file_name']=='cbscorp_sec_files.csv']\\\n",
    ".to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated_cbscorp_sec_files.csv', sep='|', encoding='utf-8')\n",
    "\n",
    "cleaned_data[cleaned_data['file_name']=='comcastcorp_sec_files.csv']\\\n",
    ".to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated_comcastcorp_sec_files.csv', sep='|', encoding='utf-8')\n",
    "\n",
    "cleaned_data[cleaned_data['file_name']=='verizoncom_sec_files.csv']\\\n",
    ".to_csv('C:/Users/li haoran/Desktop/sec files/cleaned_data_updated_verizoncom_sec_files.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Moody's is doing thie prediction themselves \n",
    "- Moddy's quant department focuse on the numbers \n",
    "- Analysit more on news and personal judgement \n",
    "- 10k predict, 8-k, 10Q twick\n",
    "- Moody's score card \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/li haoran/Desktop/sec files/cleaned_data.csv'\n",
    "cleaned_data = pd.read_csv(file_path, sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = cleaned_data[['file_name', 'doccount']]\\\n",
    ".groupby(['file_name'])\\\n",
    ".agg(['count','sum'])\\\n",
    ".reset_index()\n",
    "print(list(table1))\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(x = table1['file_name'], \n",
    "                y = table1['doccount']['count'],\n",
    "                name = 'filing count')\n",
    "\n",
    "trace2 = go.Bar(x = table1['file_name'],\n",
    "                y = table1['doccount']['sum'],\n",
    "                name = 'document count')\n",
    "\n",
    "data = [trace1,trace2]\n",
    "\n",
    "\n",
    "iplot({\n",
    "    \"data\":data,\n",
    "    \"layout\":go.Layout(title=\"Distribution of number of emails received\", \n",
    "                       xaxis={'title':'Num of emails'}, \n",
    "                       yaxis={'title':'Percentage of people received %'})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table2 = cleaned_data[['file_name','form','doccount']]\\\n",
    ".groupby(['file_name','form'])\\\n",
    ".agg(['count','sum'])\\\n",
    ".sort_values(by=['file_name'])\\\n",
    ".reset_index()\n",
    "print(list(table2))\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
