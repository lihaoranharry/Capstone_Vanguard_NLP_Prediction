{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Oct 20, 2018\n",
    "by olahosa\n",
    "\n",
    "based on jasonweinreb's WSJ scrapper: \n",
    "https://github.com/jweinreb/python-wsj/blob/master/wsj-parser.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, re, time, random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_NYT(to_search, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrape NYT for news headlines and summaries.\n",
    "    inputs:\n",
    "    - browser -> Firefox browser object (geckodriver) \n",
    "    with website with executed 1st page of search results in NYT search tool\n",
    "    - to_search -> str, search term used\n",
    "    outputs: df -> with links, dates, headline text, summary text for news\n",
    "    also saves the df as .csv\n",
    "    \"\"\"\n",
    "    \n",
    "    # get browser to do the search, provide a path to geckodriver you have donloaded here!\n",
    "    browser = webdriver.Firefox(executable_path='/Users/olahosa/Desktop/CAP/geckodriver')\n",
    "    # construct a query to pass to hte NYT search tool\n",
    "    query = '%22' + to_search.replace('&', '%26').replace(' ', '%20') + '%22'\n",
    "    # construct the website that would result from entering that query in NYT search tool\n",
    "    website = 'https://www.nytimes.com/search?endDate=' + end_date + '&query=' + query + '&sort=best&startDate=' + start_date\n",
    "    # open that website\n",
    "    browser.get(website)\n",
    "    # get the number of results as appear on NYT website after search executed\n",
    "    # use xpath to access it\n",
    "    num = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div/div[1]/p').text.split()[1]\n",
    "    # clean the result of the xpath result to be only numerical and cast to int\n",
    "    num = int(re.sub('[^0-9]','', num))\n",
    "\n",
    "    print(to_search, start_date, end_date, num)\n",
    "\n",
    "    # init lists to store data\n",
    "    links, summaries, headlines = [], [], []\n",
    "\n",
    "    i = 1\n",
    "    ad = 0\n",
    "    ad_record = [0]*6\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        time.sleep(random.randint(30,40))\n",
    "        \n",
    "        try:\n",
    "            # scroll down the page\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # wait a bit\n",
    "            time.sleep(random.randint(1,3))\n",
    "            # click on 'show more' botton\n",
    "            showmore = browser.find_element_by_class_name('Search-showMore--Plid0')\n",
    "            showmore = showmore.find_element_by_tag_name('button')\n",
    "            showmore.click()\n",
    "        # if there is no 'show more' button - got to the end of search results or error\n",
    "        except NoSuchElementException:\n",
    "            print('No more show more button')\n",
    "            break\n",
    "        # if there was 'show more'\n",
    "        else:\n",
    "            # get number of search results on the augmented page\n",
    "            element_num = len(browser.find_elements_by_xpath('//*[@id=\"site-content\"]/div/div/div[2]/div[1]/ol/li'))\n",
    "\n",
    "            # for every new item (not collected yet)\n",
    "            for j in range(i, element_num+1):\n",
    "                try: \n",
    "                    # get link, summary and headline\n",
    "                    link = browser.find_elements_by_xpath(\n",
    "                        '//*[@id=\"site-content\"]/div/div/div[2]/div[1]/ol/li['+str(j)+']/div/div/a')[0].get_attribute('href')\n",
    "                    summary = browser.find_elements_by_xpath(\n",
    "                        '//*[@id=\"site-content\"]/div/div/div[2]/div[1]/ol/li['+str(j)+']/div/div/a/p')[0].text\n",
    "                    headline = browser.find_elements_by_xpath(\n",
    "                        '//*[@id=\"site-content\"]/div/div/div[2]/div[1]/ol/li['+str(j)+']/div/div/a/h4')[0].text\n",
    "\n",
    "                    # only add newly captured stuff if it's not already in our collected list\n",
    "                    if link not in links:\n",
    "                        links.append(link)\n",
    "                        summaries.append(summary)\n",
    "                        headlines.append(headline)\n",
    "                    else:\n",
    "                        print('c!', end=' ')\n",
    "\n",
    "                # ads don't have links, summaries and headlines\n",
    "                except:\n",
    "                    ad += 1\n",
    "\n",
    "                # inc num of elements\n",
    "                i += 1\n",
    "\n",
    "                # print current time every 100 records\n",
    "                if i%100==0:\n",
    "                    print(str(datetime.datetime.now()).split(' ')[1][:5], end=', ')\n",
    "\n",
    "            # work-around for when the bot get stuck on the same page\n",
    "            # by hte number of ads recognize if anything changes\n",
    "            # and wait longer the more nothing changes\n",
    "            if ad==ad_record[-5]:\n",
    "                time.sleep(random.randint(60,70))\n",
    "                print('show more fails so sleep a bit')\n",
    "            elif ad==ad_record[-3]:\n",
    "                time.sleep(random.randint(40,50))\n",
    "                print('show more fails so sleep a bit')\n",
    "            elif ad==ad_record[-2]:\n",
    "                time.sleep(random.randint(30,40))\n",
    "                print('show more fails so sleep a bit')\n",
    "                \n",
    "            # update number of adds record for the above waiting procedure\n",
    "            ad_record.append(ad)\n",
    "\n",
    "            # print number of adds - proxy for augmentations\n",
    "            # 1 ad per a augmentation: 1 ad per 10 news\n",
    "            print(ad, end=', ')\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # POST-PROCESSING\n",
    "    \n",
    "    # print total num of ads\n",
    "    print('num ads:', ad)\n",
    "\n",
    "    # detect dates in links\n",
    "    dates = []\n",
    "\n",
    "    for i, link in enumerate(links):\n",
    "        try:\n",
    "            date = re.search(r'\\d{4}/\\d{2}/\\d{2}', str(link))[0]\n",
    "            dates.append(date)\n",
    "        except:\n",
    "            print('\\n'+link)\n",
    "            dates.append('')\n",
    "\n",
    "    print('Len of links %d, headlines %d, summaries %d and dates %d' % \n",
    "          (len(links), len(headlines), len(summaries), len(dates)))\n",
    "    print('Num unique links %d, headlines %d, summaries %d and dates %d' %\n",
    "          (len(set(links)), len(set(headlines)), len(set(summaries)), len(set(dates))))\n",
    "\n",
    "    # make df with all data\n",
    "    my_df = pd.DataFrame(\n",
    "        {'link': links,\n",
    "         'headline': headlines,\n",
    "         'summary': summaries,\n",
    "         'date': dates\n",
    "        })\n",
    "\n",
    "    # sort by date\n",
    "    my_df = my_df.sort_values(by='date')\n",
    "\n",
    "    # save df as csv\n",
    "    time_span = start_date + '_' + end_date\n",
    "    my_df.to_csv(to_search.replace(' ','-')+'_'+time_span+'_NYT_url-head-sum.csv')\n",
    "    \n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of comany names and associated lists of:\n",
    "# - search terms that were used for scraping\n",
    "# - search intervals for these search terms\n",
    "\n",
    "comp_names = ['21st-Century-Fox', 'AT&T', 'CBS', 'Comcast', 'Verizon']\n",
    "\n",
    "all_search_terms = [['News Corp', '21st Century Fox', ], \n",
    "                    ['Southwestern Bell', 'SBC', 'AT&T'],\n",
    "                    ['Viacom', 'CBS'], \n",
    "                    ['Comcast_1', 'Comcast_2'], \n",
    "                    ['Bell Atlantic', 'Verizon']]\n",
    "\n",
    "all_search_dates = [[('20031124', '20130705'), ('20130617', '20180831')],\n",
    "                    [('19930315', '19950509'), ('19941108', '20051121'), ('20051105', '20180831')],\n",
    "                    [('19930112', '20060109'), ('20060105', '20180831')],\n",
    "                    [('20011030', '20030110'), ('20021030', '20180831')],\n",
    "                    [('19930121', '20001114'), ('20000908', '20180831')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comcast 20180801 20181001 27\n",
      "2, 2, No more show more button\n",
      "num ads: 2\n",
      "Len of links 20, headlines 20, summaries 20 and dates 20\n",
      "Num unique links 20, headlines 20, summaries 20 and dates 13\n",
      "CBS 20180802 20181011 210\n",
      "2, 3, 4, 5, 6, 7, 8, 21:02, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21:09, 19, 20, 21, No more show more button\n",
      "num ads: 21\n",
      "\n",
      "https://www.nytimes.com/video/opinion/100000005884108/trump-maga-not-the-way-he-thinks.html\n",
      "\n",
      "https://www.nytimes.com/paidpost/cbs/why-are-murder-rates-so-high-in-the-rust-belt.html\n",
      "\n",
      "https://www.nytimes.com/video/opinion/100000005831656/earthrise.html\n",
      "Len of links 210, headlines 210, summaries 210 and dates 210\n",
      "Num unique links 210, headlines 208, summaries 192 and dates 63\n"
     ]
    }
   ],
   "source": [
    "# for each company (and associated search terms and search time)\n",
    "for comp_name, search_terms, search_dates in zip(comp_names, all_search_terms, all_search_dates):\n",
    "    \n",
    "    for search_term, search_date in zip(search_terms, search_dates):\n",
    "        \n",
    "        start_date = search_date[0]\n",
    "        end_date = search_date[1]\n",
    "        \n",
    "        # work-around to deal with the fact that search terms for some companies are the same\n",
    "        if search_term[-2] == '_':\n",
    "            to_search = search_term[:-2]\n",
    "        else:\n",
    "            to_search = search_term\n",
    "        \n",
    "        scrape_NYT(to_search, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
